{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c142cfb-78f5-4126-8f67-b1735786473b",
   "metadata": {},
   "source": [
    "# Generating Synthetic Datasets for Evaluating Retrieval Augmented Generation Systems\n",
    "\n",
    "\n",
    "As Retrieval Augmented Generation (RAG) systems become more prevalent, evaluating their performance is essential to ensure quality and performace. However, collecting real-world data for evaluation can be costly and time-consuming, especially in the early stages of a project. To addresses this challenge of data scarcity, synthetic dataset generation provides a practical solution for generating datasets that mimic real human interactions, enabling efficient and scalable evaluation of RAG systems. By leveraging large language models and knowledge retrieval context, the proposed approach ensures that the synthetic datasets are diverse, realistic, and representative of real-world scenarios. This solution is relevant for developers and researchers working on RAG systems, as it streamlines the evaluation process and accelerates the iterative development cycle, ultimately leading to better-performing AI systems. The process of generating synthetic datasets is integrated in open source tools like [RAGAS](https://docs.ragas.io/en/stable/) and will be outlined in this notebook.  \n",
    "\n",
    "In this notebook you will be guided through generating a synthetic dataset for a QA-RAG application using Anthropic Claude via the Bedrock API, Python and Langchain. The notebook consists of the following chapters: \n",
    "\n",
    "1. [Set-up of the environment](#1.-Set-up-of-the-environment)\n",
    "2. [Loading and preparing context data](#2-loading-and-preparing-data)\n",
    "3. [Initial Question Generation](#3.-Initial-Question-Generation)\n",
    "4. [Answer Generation](#4.-Answer-Generation)\n",
    "5. [Extracting Relevant Context](#5.-Extracting-Relevant-Context)\n",
    "6. [Evolving Questions to fit End-User behaviour](#6.-Evolving-Questions-to-fit-end-users-behaviour)\n",
    "7. [Automated Dataset Generation](#7.-Automated-Dataset-Generation)\n",
    "8. [Assessing the questions quality using Critique Agents](#8-assessing-the-questions-quality-using-critique-agents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13666ed4-1d44-43d3-a59e-b0db38cf4520",
   "metadata": {},
   "source": [
    "## 1. Set-up of the environment\n",
    "Let's start by installing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2f16b-e59a-4155-92c1-dde7eee6f2f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q langchain==0.1.10 boto3 pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bb4a4-bc7e-4db7-9cce-2062494cfb95",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Data\n",
    "\n",
    "For this lab you will use a fictuous use case where you want to build a chatbot to answer questions about Amazon shareholder letters. A typical technique to build such a chatbot is  Retrieval-Augmented Generation (RAG). While this lab focuses on dataset generation, let's start with a quick RAG primer for some background context. \n",
    "\n",
    "#### What is RAG?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. \n",
    "\n",
    "Now in order to build a synthetic training dataset for such a question answering RAG system, raw data from the knowledge source is used to derive possible user questions. For our use case you will use PDF files of shareholder letters with text information loaded from the interent to serve as the knowledge base. In production grade RAG implementations, the knowledge retriever may leverage a database that supports vector searches to dynamically lookup relevant documents that serve as the knowledge source.\n",
    "\n",
    "In our case let's start by downloading the shareholder letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8a5d5-d361-4dc2-bcb4-c0ae01456e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for downloading files\n",
    "from urllib.request import urlretrieve \n",
    "import os\n",
    "\n",
    "# Create folder to store downloaded files\n",
    "# Use descriptive folder name relating to data\n",
    "folder_name = \"synthetic_dataset_generation\"  \n",
    "\n",
    "# Check if folder already exists, if yes do nothing\n",
    "# If no, create the folder\n",
    "if os.path.exists(folder_name):\n",
    "    pass  \n",
    "else:\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "# List of URLs of files to download\n",
    "files = [\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',  \n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
    "]\n",
    "\n",
    "# Iterate through list of URLs \n",
    "for url in files:\n",
    "\n",
    "    # Get file name from URL to use as local file name\n",
    "    file_path = os.path.join(\"synthetic_dataset_generation\", url.rpartition(\"/\")[2])  \n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10bbcc-47b5-4ba0-a0b6-e38f4bd79abc",
   "metadata": {},
   "source": [
    "Now that the context data in the form of the shareholder letters has been downloaded you now load PDF documents from the created directory, you will now split them into smaller text chunks using a recursive character text splitter from the Langchain library. The RecursiveCharacterTextSplitter divides the text into chunks of a specified size while allowing for overlap to prevent cutting sentences in half. When setting the chunk size, make sure it fits into the context window of your LLM and feel free to experiment with different chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00371a36-ceef-44fe-828a-95489bf7bae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.pdf import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "\n",
    "# Load PDF documents from directory\n",
    "loader = PyPDFDirectoryLoader(\"./synthetic_dataset_generation/\")  \n",
    "documents = loader.load()\n",
    "\n",
    "# Use recursive character splitter, works better for this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    # Split documents into small chunks\n",
    "    chunk_size = 1500,  \n",
    "\n",
    "    # Overlap chunks to reduce cutting sentences in half\n",
    "    chunk_overlap  = 100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Split loaded documents into chunks\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c9c3a-915c-495b-8fac-8b1cfe97e4b3",
   "metadata": {},
   "source": [
    "Let's have a look at the size of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831439b9-f018-4f2c-91e1-0bd4ec58b8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print metadata of the loaded documents\n",
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} pages loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split you have {len(docs)}')\n",
    "print(f'Average length among {len(docs)} chunks is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c6c15-1985-4e96-9391-6f1bbfc4b9a6",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  As Amazon Bedrock will be used for generating synthetic data in the following you will connect to the Bedrock API. Further in the Lab you will use the Langchain library to communicate with the Amazon Bedrock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381cc96-fcf5-4510-bfff-63e0a4b38fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up Amazon Bedrock as LLM supplier for synthetic dataset creation\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "# set up a Bedrock-runtime client for inferencing large language models\n",
    "boto3_bedrock = boto3.client('bedrock-runtime')\n",
    "\n",
    "# set up a Bedrock client for performing administrative API calls\n",
    "boto3_bedrock_admin = boto3.client('bedrock')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "# Choosing claude 3 Haiku due to cost and performance efficiency\n",
    "claude_3_haiku = \"anthropic.claude-3-haiku-20240307-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed135f-987c-4204-a9a1-0140397b9a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set-up langchain LLM for implementing the synthetic dataset generation logic\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "# for each model provider there are different parameters to define when inferencing against the model\n",
    "inference_modifier = {\n",
    "                        \"max_tokens\": 10000,\n",
    "                        \"temperature\": 0.5,\n",
    "                        \"top_k\": 250,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                     \n",
    "\n",
    "llm = BedrockChat(model_id = claude_3_haiku,\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaba95-a7fa-4c09-81f1-90f826246109",
   "metadata": {},
   "source": [
    "## 3. Initial Question Generation\n",
    "\n",
    "As a first step you generate sample questions. You can use each of the generated chunks to generate synthetic questions that a real chatbot user might ask. You will prompt the LLM to analyze a chunk of shareholder letter data and generate a relevant question based on the information presented in the context. Below is a sample prompt to generate a question given a specific context. Note you are hardcoding to generate a single question for simplicity, of course you can also ask the LLM to generate multiple questions with a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457b697-468a-44b2-a1c8-ba92ddbd4632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template to generate a question a end-user could have about a given context\n",
    "initial_question_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=\"\"\"\n",
    "    <Instructions Structure>\n",
    "    1. Provide context\n",
    "    2. Explain the task and rules\n",
    "    3. Instruct to generate a question based on the context following the rules\n",
    "    4. Specify output format\n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    Here is some context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Your task is to generate 1 question that can be answered using the provided context, following these rules:\n",
    "\n",
    "    <rules>\n",
    "    1. The question should make sense to humans even when read without the given context.\n",
    "    2. The question should be fully answered from the given context.\n",
    "    3. The question should be framed from a part of context that contains important information. It can also be from tables, code, etc.\n",
    "    4. The answer to the question should not contain any links.\n",
    "    5. The question should be of moderate difficulty.\n",
    "    6. The question must be reasonable and must be understood and responded by humans.\n",
    "    7. Do not use phrases like 'provided context', etc. in the question.\n",
    "    8. Avoid framing questions using the word \"and\" that can be decomposed into more than one question.\n",
    "    9. The question should not contain more than 10 words, make use of abbreviations wherever possible.\n",
    "    </rules>\n",
    "\n",
    "    To generate the question, first identify the most important or relevant part of the context. Then frame a question around that part that satisfies all the rules above.\n",
    "\n",
    "    <thinking>\n",
    "    [This is a space for you to write down your thoughts and identify relevant parts of the context as you formulate the question.]\n",
    "    </thinking>\n",
    "\n",
    "    Output only the generated question with a \"?\" at the end, no other text or characters.\n",
    "    </Instructions>\n",
    "    \n",
    "    \"\"\")\n",
    "\n",
    "def generate_question(doc, llm):\n",
    "\n",
    "    # Pass in values to the input variables\n",
    "    initial_question_prompt = initial_question_prompt_template.format(context=doc)\n",
    "    \n",
    "    initial_question = llm.invoke(initial_question_prompt)\n",
    "    \n",
    "    return initial_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d6c90-18f0-4ae3-8843-37b31bf9db7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a question based on a given context\n",
    "question = generate_question(docs[1], llm)\n",
    "print(f\"Intial question: {question.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466639dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e8a8e-c8a1-487a-8d12-3ce024f67a69",
   "metadata": {},
   "source": [
    "## 4. Answer Generation\n",
    "To use the questions for evaluation you need to generate a reference answer for each of the questions to test against. Let's do this using following prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca873f-ddd2-4b51-987d-5524d198c56c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that takes into consideration the the question and generates an answer\n",
    "answer_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    <Instructions Structure>\n",
    "    1. Provide context\n",
    "    2. State the task and rules\n",
    "    3. Provide the question\n",
    "    4. Instruct to generate the answer based only on the given context\n",
    "    5. Instruct to output only the generated answer sentence\n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    <Task>\n",
    "    <role>You are an experienced QA Engineer for building large language model applications.</role>\n",
    "    <task>It is your task to generate an answer to the following question <question>{question}</question> only based on the <context>{context}</context></task>\n",
    "    The output should be only the answer generated from the context.\n",
    "\n",
    "    <rules>\n",
    "    1. Only use the given context as a source for generating the answer.\n",
    "    2. Be as precise as possible with answering the question.\n",
    "    3. Be concise in answering the question and only answer the question at hand rather than adding extra information.\n",
    "    </rules>\n",
    "\n",
    "    Only output the generated answer as a sentence. No extra characters.\n",
    "    </Task>\n",
    "    </Instructions>\n",
    "    \n",
    "    Assistant:\"\"\")\n",
    "\n",
    "def generate_answer(question: str, doc, llm):\n",
    "    \n",
    "    answer_prompt = answer_prompt_template.format(question = question, context=doc)\n",
    "    \n",
    "    answer = llm.invoke(answer_prompt)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffceef3c-9f53-4143-b5c9-0b80f3e6d182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = generate_answer(question, docs[1], llm)\n",
    "print(f\"Intial question: {question.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Reference Answer: {answer.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ddc9b-f409-4655-a943-361939fa4946",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Extracting Relevant Context\n",
    "To make the dataset verifiable you use the following prompt to extract the relevant sentences from the given context to answer the generated question. Knowing the relevant sentences you can easyly check whether the question and answer are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d1f19-d1af-4317-b9cb-e4142700939d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To check whether an answer was correctly formulated by the large language model you get the relevant text passages from the documents used for answering the questions.\n",
    "source_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Human:\n",
    "    <Instructions Structure>\n",
    "    1. Provide the context\n",
    "    2. State the task of extracting relevant sentences from the context to answer the question\n",
    "    3. Provide the question\n",
    "    4. Instruct to output only the relevant sentences, without any extra characters or explanations\n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    Here is the context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Your task is to extract the relevant sentences from the given context that can potentially help answer the following question. You are not allowed to make any changes to the sentences from the context.\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Output only the relevant sentences you found, one sentence per line, without any extra characters or explanations.\n",
    "    </Instructions>\n",
    "    Assistant:\"\"\")\n",
    "\n",
    "def generate_source(question: str, doc, llm):\n",
    "        \n",
    "    source_prompt = source_prompt_template.format(question = question, context=doc)\n",
    "    \n",
    "    source = llm.invoke(source_prompt)\n",
    "    \n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008571ba-6cde-40a0-833b-0348d54e9a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_sentence = generate_source(question, docs[1], llm)\n",
    "print(f\"Intial question: {question.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Reference Answer: {answer.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Source Sentence: {source_sentence.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45065c6-7387-42fe-878d-884f6d297499",
   "metadata": {},
   "source": [
    "## 6. Evolving Questions to fit end-users behaviour\n",
    "When generating question & answer pairs from the same prompt for the whole dataset it might appear that the questions are repetetive, similar in form and thus not mimic real enduser behaviour. In this section you evolve the existing generated question to for example make it shorter and more precise. The prompt for generating questions that fit your use case heavily depend on your use case and thus your prompt must reflect your endusers by for instance setting the rules accordingly or by providing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6c408-6395-4d8a-b217-a1a45bae02e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To generate a more versatile testing dataset you alternate the questions to see how your RAG systems performs against differently formulated of questions\n",
    "question_compress_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    <Instructions Structure>\n",
    "    1. Provide context on the task\n",
    "    2. State the rules for rewriting the question\n",
    "    3. Instruct to output only the rewritten question with a question mark\n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    <role>You are an experienced linguistics expert for building testsets for large language model applications.</role>\n",
    "\n",
    "    <task>It is your task to rewrite the following question in a more indirect and compressed form, following these rules:\n",
    "\n",
    "    <rules>\n",
    "    1. Make the question more indirect\n",
    "    2. Make the question shorter\n",
    "    3. Use abbreviations if possible\n",
    "    </rules>\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Your output should only be the rewritten question with a question mark \"?\" at the end. Do not provide any other explanation or text.\n",
    "    </task>\n",
    "    </Instructions>\n",
    "    \n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def compress_question(question): \n",
    "    # Pass in values to the input variables\n",
    "    question_compress_prompt = question_compress_prompt_template.format(question=question)\n",
    "    \n",
    "    question_compressed = llm.invoke(question_compress_prompt)\n",
    "        \n",
    "    return question_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b05eb8-d9da-42b9-8b00-e69f9fc79907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compressed_question = compress_question(question)\n",
    "print(f\"Intial question: {question.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Reference Answer: {answer.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Source Sentence: {source_sentence.content}\")\n",
    "print(\"---\")\n",
    "print(f\"Compressed Question: {compressed_question.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50011c1a-ff3a-4ecd-b715-c69db89905a8",
   "metadata": {},
   "source": [
    "## 7. Automated Dataset Generation\n",
    "To scale the process of the dataset generation you iterate over all chunks of your context, generate questions, answers, relevant sentences and evolutions for each chunk and save them to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7198b44-886e-4538-a23d-d9c642c4632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subset of the loaded documents for lightweight testing. For generating data for every document please work with docs.\n",
    "docs_subset = docs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac8015-3787-4d26-a024-3da2d2a189a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "def generate_qa_dataset_doc(doc: Document, llm, dataset, doc_number):\n",
    "    \"\"\"A function to create a test dataset of questions for a given Document(Langchain Document type)\"\"\"\n",
    "    \n",
    "    # generate the initial question for the RAG testdataset\n",
    "    question = generate_question(doc, llm)\n",
    "    dataset.at[doc_number, \"question\"] = question.content\n",
    "    \n",
    "    # generate compressed  question to variate the dataset\n",
    "    compressed_question = compress_question(question)\n",
    "    dataset.at[doc_number, \"question_compressed\"] = compressed_question.content\n",
    "   \n",
    "    \n",
    "    answer = generate_answer(question, doc, llm)\n",
    "    dataset.at[doc_number, \"reference_answer\"] = answer.content\n",
    "        \n",
    "    source_sentence = generate_source(question, doc, llm)\n",
    "    dataset.at[doc_number, \"source_sentence\"] = source_sentence.content\n",
    "    \n",
    "    source_raw = doc\n",
    "    dataset.at[doc_number, \"source_raw\"] = source_raw.page_content\n",
    "    \n",
    "    source_document = doc.metadata[\"source\"]\n",
    "    dataset.at[doc_number, \"source_document\"] = source_document\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c30fb8-aa99-4c6a-8c91-f449c89bb103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataset class that in the end can be used to generate the dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dataset = pd.DataFrame(columns=[\"question\", \"question_compressed\", \"reference_answer\", \"source_sentence\",\"source_raw\",\"source_document\" ])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5d096-421d-4c06-bb03-2f57df9710c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_dataset(documents: Document,llm, dataset):\n",
    "\n",
    "    print(f\"start generating dataset from {len(documents)} docuements\")\n",
    "    print(\"---\")\n",
    "    generation_time_start = time.time()\n",
    "    \n",
    "    for doc in tqdm(range(len(documents))):\n",
    "        q_generation_time_start = time.time()\n",
    "        dataset = generate_qa_dataset_doc(doc = documents[doc], llm = llm, dataset = dataset, doc_number = doc)\n",
    "        q_generation_time_end = time.time()\n",
    "        total_elapsed_time_generation = q_generation_time_end - q_generation_time_start\n",
    "\n",
    "\n",
    "        print(f\"Finished creating evaluation data for chunk {doc+1}\")\n",
    "        print(f\"Generation time for doc: {total_elapsed_time_generation}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "    generation_time_end = time.time()\n",
    "    total_elapsed_time= generation_time_end - generation_time_start\n",
    "    print(f\"Generation time for all docs: {total_elapsed_time}\")\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac63dc-b676-477b-8bc7-9aa750732f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_df = generate_dataset(docs_subset, llm, dataset)\n",
    "\n",
    "num_questions_generated = dataset_df.shape[0]\n",
    "print(f\"Generated a total of {num_questions_generated} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b80a38-1b5c-47a1-8e4c-a357f4d9dd8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the first rows of the generated dataset\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499b380-7680-40c5-a25e-34edfd0916f6",
   "metadata": {},
   "source": [
    "## 8. Assessing the questions quality using Critique Agents\n",
    "Critique agents are a technique used in natural language processing (NLP) to evaluate the quality and suitability of questions in a dataset for a particular task or application. In this case, the critique agents are employed to assess whether the questions in a dataset are valid for a Retrieval-Augmented Generation (RAG) system, which is a type of language model that combines information retrieval and generation capabilities.\n",
    "\n",
    "The two main metrics evaluated by the critique agents are relevance and groundness.\n",
    "\n",
    "Relevance\n",
    "\n",
    "Relevance measures how useful and applicable a question is for a specific domain or context. In the context of financial and business analysis, the relevance prompt evaluates questions based on the following criteria:\n",
    "\n",
    "- Is the question directly relevant to the work of financial and business analysts on Wall Street?\n",
    "- Does the question address a practical problem or use case that analysts might encounter?\n",
    "- Is the question clear and well-defined, avoiding ambiguity or vagueness?\n",
    "- Does the question require a substantive answer that demonstrates understanding of financial topics?\n",
    "- Would answering the question provide insights or knowledge that could be applied to real-world company evaluation tasks?\n",
    "\n",
    "The relevance score ranges from 1 to 5, with a higher score indicating greater relevance and usefulness for financial and business analysts.\n",
    "\n",
    "Groundness\n",
    "\n",
    "Groundness measures how well a question can be answered based on the provided context or information. The groundness prompt evaluates questions based on the following criteria:\n",
    "\n",
    "- Can the question be answered using only the information provided in the given context?\n",
    "- Does the context provide little, some, substantial, or all the information needed to answer the question?\n",
    "\n",
    "The groundness score also ranges from 1 to 5, with the following interpretations:\n",
    "\n",
    "1. The question cannot be answered at all based on the given context.\n",
    "2. The context provides very little relevant information to answer the question.\n",
    "3. The context provides some relevant information to partially answer the question.\n",
    "4. The context provides substantial information to answer most aspects of the question.\n",
    "5. The context provides all the information needed to fully and unambiguously answer the question.\n",
    "\n",
    "By evaluating both relevance and groundness, the critique agents can help identify questions in the dataset that are well-suited for the RAG system, as well as those that may need to be revised, removed, or supplemented with additional context or information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a5af5-2e80-49df-b223-907729b1b707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groundness_check_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=\"\"\"\n",
    "\n",
    "    <Instructions Structure>\n",
    "    1. Provide context\n",
    "    2. Ask for evaluation/reasoning\n",
    "    3. Ask for total rating score \n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    You will be given a context and a question related to that context.\n",
    "\n",
    "    Your task is to provide an evaluation of how well the given question can be answered using only the information provided in the context. Rate this on a scale from 1 to 5, where:\n",
    "\n",
    "    1 = The question cannot be answered at all based on the given context\n",
    "    2 = The context provides very little relevant information to answer the question\n",
    "    3 = The context provides some relevant information to partially answer the question \n",
    "    4 = The context provides substantial information to answer most aspects of the question\n",
    "    5 = The context provides all the information needed to fully and unambiguously answer the question\n",
    "\n",
    "    First, read through the provided context carefully:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Then read the question:\n",
    "\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Evaluate how well you think the question can be answered using only the context information. Provide your reasoning first in an <evaluation> section, explaining what relevant or missing information from the context led you to your evaluation score in only one sentence.\n",
    "\n",
    "    Provide your evaluation in the following format:\n",
    "\n",
    "    <rating>(Your rating from 1 to 5)</rating>\n",
    "    \n",
    "    <evaluation>(Your evaluation and reasoning for the rating)</evaluation>\n",
    "\n",
    "\n",
    "    </Instructions>\n",
    "    \n",
    "    \"\"\")\n",
    "\n",
    "relevance_check_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    <Instructions Structure>\n",
    "    1. Explain the task to the AI assistant\n",
    "    2. Provide guidelines for evaluating the usefulness of the question\n",
    "    3. Ask the AI to provide its evaluation and rating, with the rating first followed by the justification in only one sentence\n",
    "    </Instructions Structure>\n",
    "\n",
    "    <Instructions>\n",
    "    You will be given a question related to Amazon Shareholder letters. Your task is to evaluate how useful this question would be for a financial and business analyst working in wallstreat.\n",
    "\n",
    "    To evaluate the usefulness of the question, consider the following criteria:\n",
    "\n",
    "    1. Relevance: Is the question directly relevant to your work? Questions that are too broad or unrelated to this domain should receive a lower rating.\n",
    "\n",
    "    2. Practicality: Does the question address a practical problem or use case that analysts might encounter? Theoretical or overly academic questions may be less useful.\n",
    "\n",
    "    3. Clarity: Is the question clear and well-defined? Ambiguous or vague questions are less useful.\n",
    "\n",
    "    4. Depth: Does the question require a substantive answer that demonstrates understanding of financila topics? Surface-level questions may be less useful.\n",
    "\n",
    "    5. Applicability: Would answering this question provide insights or knowledge that could be applied to real-world company evaluation tasks? Questions with limited applicability should receive a lower rating.\n",
    "\n",
    "    Provide your evaluation in the following format:\n",
    "\n",
    "    <rating>(Your rating from 1 to 5)</rating>\n",
    "    \n",
    "    <evaluation>(Your evaluation and reasoning for the rating)</evaluation>\n",
    "\n",
    "    Here is the question:\n",
    "\n",
    "    {question}\n",
    "    </Instructions>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69431e-6b5a-46a6-8166-cbaf2fb6a663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_groundness_check(question, source_raw): \n",
    "    # Pass in values to the input variables\n",
    "    groundness_prompt = groundness_check_prompt_template.format(question=question, context=source_raw)\n",
    "    \n",
    "    groundness_rating = llm.invoke(groundness_prompt)\n",
    "        \n",
    "    return groundness_rating\n",
    "\n",
    "def generate_relevance_check(question): \n",
    "    # Pass in values to the input variables\n",
    "    relevance_prompt = relevance_check_prompt_template.format(question=question)\n",
    "    \n",
    "    relevance_rating = llm.invoke(relevance_prompt)\n",
    "        \n",
    "    return relevance_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941864ce-49c2-4800-aff5-203b24ee6816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluating one of the generated questions for groundness and relevance\n",
    "groundness_rating = generate_groundness_check(dataset_df.question[0], dataset_df.source_raw[0])\n",
    "relevance_rating = generate_relevance_check(dataset_df.question[0])\n",
    "\n",
    "print(\"Groundness Score:\")\n",
    "print(groundness_rating.content)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(\"Relevance Score:\")\n",
    "print(relevance_rating.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18dc75-4424-4112-b1dc-dca4dc3c41d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Helper functions to extract values from the string response by the LLM Critique Agents.\n",
    "def extract_rating(text):\n",
    "    pattern = r'<rating>(.*?)</rating>'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        rating = match.group(1)\n",
    "        return rating\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def extract_reasoning(text):\n",
    "    pattern = r'<evaluation>(.*?)</evaluation>'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        rating = match.group(1)\n",
    "        return rating\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset):\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        question = row['question']\n",
    "        source_raw = row['source_raw']\n",
    "\n",
    "        # Generate groundness check\n",
    "        groundness_check = generate_groundness_check(question, source_raw)\n",
    "        groundness_score = extract_rating(groundness_check.content)\n",
    "        groundness_score_reasoning = extract_reasoning(groundness_check.content)\n",
    "\n",
    "        dataset.at[index, 'groundness_score'] = groundness_score\n",
    "        dataset.at[index, 'groundness_score_reasoning'] = groundness_score_reasoning\n",
    "\n",
    "        # Generate relevance check\n",
    "        relevance_check = generate_relevance_check(question)\n",
    "        relevancy_score = extract_rating(relevance_check.content)\n",
    "        relevancy_score_reasoning = extract_reasoning(relevance_check.content)\n",
    "\n",
    "        dataset.at[index, 'relevancy_score'] = relevancy_score\n",
    "        dataset.at[index, 'relevancy_score_reasoning'] = relevancy_score_reasoning\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1e46c",
   "metadata": {},
   "source": [
    "Now that the concept of critique agents has been established including the prompt for groundness and relevance scores you iterate over the generated dataset dataset and assign each question a score. Depending on your needs you can eliminate questions with a score beneath a certain threshold from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec660fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_evaluated = evaluate_dataset(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_evaluated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497224ae",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Generating synthetic datasets is a powerful technique for evaluating retrieval augmented generation (RAG) systems, particularly in the early stages of development when real-world data is scarce or difficult to obtain. By leveraging large language models and knowledge retrieval context, this approach enables the creation of diverse, realistic, and representative datasets that mimic real human interactions.\n",
    "\n",
    "Throughout this notebook, you have explored the process of generating a synthetic dataset for a QA-RAG application using Anthropic's Claude via the Bedrock API, Python, and Langchain. You covered essential steps, including setting up the environment, loading and preparing context data, initial question generation, answer generation, extracting relevant context, evolving questions to fit end-user behavior, automated dataset generation, and assessing question quality.\n",
    "\n",
    "While this approach offers numerous benefits, it is essential to acknowledge its limitations. First, the quality of the synthetic dataset heavily relies on the performance and capabilities of the underlying language model and knowledge retrieval system. Biases and limitations present in these components may be reflected in the generated dataset. Additionally, capturing the full complexity and nuances of real-world interactions can be challenging, as synthetic datasets may not account for all edge cases or unexpected scenarios.\n",
    "\n",
    "Despite these limitations, generating synthetic datasets remains a valuable tool for accelerating the development and evaluation of RAG systems. By streamlining the evaluation process and enabling iterative development cycles, this approach can contribute to the creation of better-performing AI systems.\n",
    "\n",
    "We encourage developers, researchers, and enthusiasts to explore the open-source tools like RAGAS mentioned in this notebook and experiment with generating synthetic datasets for their own RAG applications. Hands-on experience with this technique can provide valuable insights and contribute to the advancement of RAG systems in various domains.\n",
    "\n",
    "Remember, synthetic dataset generation is not a silver bullet, but rather a powerful tool that should be used in conjunction with other evaluation techniques and real-world data when available. By embracing this approach and continuously improving upon it, you can accelerate the development of more robust and capable RAG systems, ultimately enhancing the user experience and unlocking new possibilities in natural language processing."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
